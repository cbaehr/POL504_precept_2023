# create plot to illustrate zipf's law
plot(log(1:100), log(topfeatures(dfm, 100)),
xlab="log(rank)", ylab="log(frequency)", main="Top 100 Words")
abline(regression, col="red")
abline(a = regression$coefficients["(Intercept)"], b = -1, col = "black")
num_tokens <- sum(rowSums(dfm))
M <- nfeat(dfm)
k <- 44
# solve for b
b <- log(M/k)/log(num_tokens)
b
corpus <- corpus_subset(data_corpus_ukmanifestos, Year %in% c(1945:1955))
corpus
docvars(corpus)
# key words in context
corpus_subset(corpus, party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
# key words in context
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus <- tolower(corpus)
# key words in context
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus <- corpus_subset(data_corpus_ukmanifestos, Year %in% c(1945:1955))
corpus <- tolower(corpus)
# key words in context
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus <- corpus_subset(data_corpus_ukmanifestos, Year %in% c(1945:1955))
corpus <- tolower(corpus)
# key words in context
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
topfeatures(corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T))
topfeatures(corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>% dfm())
topfeatures(corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>% tokens_remove(stopwords()) %>% dfm())
topfeatures(corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>% tokens_remove(stopwords()) %>% dfm(), 100)
grep("churchil", corpus)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("interest", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("interest", window = 5)
sotu.sub <- sotu[which(sotu$year %in% 1982:2020),]
sotu.tokens <- tokens(sotu.sub)
sotu.sub <- corpus_reshape(sotu.sub, "sentence")
sotu.tokens <- tokens(sotu.sub)
sotu.sub <- data.frame(corpus_reshape(sotu.sub, "sentence"))
View(sotu.sub)
sotu.sub <- corpus_reshape(sotu.sub, "sentence")
sotu.sub <- sotu[which(sotu$year %in% 1982:2020),]
sotu.sub <- corpus_reshape(sotu.sub, "sentence")
docvars(sotu.sub)
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub["year"])) |>
setNames("text", "year")
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub["year"])) |>
setNames(c("text", "year"))
docvars(sotu.sub["year"])
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"]) |>
setNames(c("text", "year"))
sotu.split <- split(sotu.sub, as.factor(sotu.df$year))
View(sotu.split)
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
View(sotu.split)
docvars(sotu.sub)
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"]) |>
setNames(c("text", "year", "sotu_type"))
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"], docvars(sotu.sub)["sotu_type"]) |>
setNames(c("text", "year", "type"))
sotu.split <- split(sotu.sub, as.factor(sotu.df$year))
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
View(sotu.split)
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"], docvars(sotu.sub)["sotu_type"]) |>
setNames(c("text", "year", "type"))
sotu.split <- split(sotu.df, as.factor(sotu.df$year))
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"]) |>
setNames(c("text", "year"))
sotu.split <- split(sotu.df, as.factor(sotu.df$year))
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
docs.boot <- corpus(year[docnums, "text"])
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- corpus(year[docnums, "text"])
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
iter <- 10 # NUMBER OF BOOTSTRAP SAMPLES (usually would want more, >=100)
## for loop to compute as many samples as specified
for(i in 1:iter) {
if(i==1) {boot.means <- list()} # generate new list
# store the results in new element i
boot.means[[i]] <- lapply(budget.lp.df.SPLIT, boot.fre)
print(i)
}
iter <- 10 # NUMBER OF BOOTSTRAP SAMPLES (usually would want more, >=100)
## for loop to compute as many samples as specified
for(i in 1:iter) {
if(i==1) {boot.means <- list()} # generate new list
# store the results in new element i
boot.means[[i]] <- lapply(sotu.split, boot.fre)
print(i)
}
## combine the point estimates to a data frame and compute statistics by party
boot.means.df <- do.call(rbind.data.frame, boot.means)
mean.boot <- apply(boot.means.df, 2, mean)
sd.boot <- apply(boot.means.df, 2, sd)
## create data frame for plot
plot_df <- data.frame(sotu.df$year, mean.boot, sd.boot) |>
setNames(c("year", "mean", "se"))
sotu.df$year
mean.boot
## create data frame for plot
plot_df <- data.frame(sort(unique(sotu.df$year)), mean.boot, sd.boot) |>
setNames(c("year", "mean", "se"))
## confidence intervals
ci90 <- qnorm(0.95)
ci95 <- qnorm(0.975)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
geom_linerange(aes(x = year,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = year,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
theme(legend.position = "none") # fancy stuff
pacman::p_load(corpus, dplyr, ggplot2, gutenbergr, quanteda, quanteda.corpora, quanteda.textstats,
readtext, rperseus, sotu, stringr, stylest, text.alignment)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
geom_linerange(aes(x = year,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = year,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
theme(legend.position = "none") # fancy stuff
flesch_point <- sotu.df$text %>%
textstat_readability(measure = "Flesch") %>%
group_by(sotu.df$year) %>%
summarise(mean_flesch = mean(Flesch)) %>%
setNames(c("year", "mean")) %>%
arrange(as.numeric(year))
cbind(flesch_point, "bs_mean" = plot_df$mean)
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "Dale.Chall"))
# compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$Dale.Chall))
# print
rownames(readability_cor) <- c("Flesch", "Dale-Chall")
colnames(readability_cor) <- c("Flesch", "Dale-Chall")
readability_cor
docs <- corpus( readtext(paste0("/Users/christianbaehr/Documents/GitHub/pol504_private/homework1/", c("melania", "michelle"), ".txt")) )
# set gap to default (-1)
sw2 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words", gap=-1)
# increase gap penalty to -5 --> reduces extent of plagiarism. Why?
sw3 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words", gap=-5)
sw2
sw3
View(sw2)
sw2$sw
sw3$sw
help("textstat_readability")
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "FOG"))
View(fre_and_dc_measures)
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "FOG"))
# compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$FOG))
# print
rownames(readability_cor) <- c("Flesch", "FOG")
colnames(readability_cor) <- c("Flesch", "FOG")
readability_cor
rm(list = ls())
setwd("/Users/christianbaehr/Documents/GitHub/pol504_private/homework1/")
pacman::p_load(corpus, dplyr, ggplot2, gutenbergr, quanteda, quanteda.corpora, quanteda.textstats,
readtext, rperseus, sotu, stringr, stylest, text.alignment)
R.Version()
library(sf)
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties
gsub("\t", "", counties)
counties <- gsub("\t", "", counties)
gsub("<option value=\", "", counties)
)
sdfa
)))
((()))
(
>
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('<option value=\\\\', "", counties)
counties
gsub('<option value=([\\])', "", counties)
counties <- gsub('<option value=', "", counties)
counties
counties <- gsub('option>', "", counties)
counties
gsub('"', '', counties)
gsub('"|<', '', counties)
gsub('"|<|>', '', counties)
gsub('"|<|>|\\d', '', counties)
gsub('"|<|>|\\d|//', '', counties)
gsub('"|<|>|\\d|////', '', counties)
gsub('"|<|>|\\d|///', '', counties)
gsub('"|<|>|\\d|', '', counties)
counties <- gsub('"|<|>|\\d|', '', counties)
counties
counties <- gsub('////', '', counties)
counties
gsub('////', '', counties)
counties <- gsub("([\\])","", counties)
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|select', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties
gsub("([/])","", counties)
gsub("/","", counties)
counties <- trimws(gsub("/","", counties))
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|select|eded', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties <- trimws(gsub("/","", counties))
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|select|eded', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties <- trimws(gsub("/","", counties))
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|selected', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties <- trimws(gsub("/","", counties))
counties
write.table(counties, "/Users/christianbaehr/Desktop/county_names.txt", row.names = F)
## working directory
setwd("/Users/christianbaehr/Documents/GitHub/POL504_precept_2023/")
pacman::p_load(quanteda, quanteda.corpora, readtext, quanteda.textmodels,
quanteda.textplots, dplyr)
news_data <- readRDS("data/news_data.rds")
View(news_data)
news_data <- news_data[, c("category", "headline")] # keep relevant columns
## light pre-processing
news_data$headline <- gsub("'", "", news_data$headline) # remove apos
news_corpus <- news_data |>
corpus(text_field = "headline") |>
corpus_subset(category %in% c("SPORTS", "ARTS"))
arts <- corpus_subset(news_corpus, category == "ARTS")
ntokens(arts)
arts <- corpus_subset(news_corpus, category == "ARTS") |>
tokens()
ntokens(arts)
ntoken(arts)
sum(ntoken(arts))
arts <- corpus_subset(news_corpus, category == "ARTS") |>
tokens() |>
dfm()
dfm_select(arts, "artist")
sum(dfm_select(arts, "artist"))
ntoken(arts)
sum(dfm_select(arts, "artist")) / sum(ntoken(arts))
P_A__B <- sum(dfm_select(arts, "artist")) / sum(ntoken(arts))
length(news_data)
length(news_corpus)
P_B <- nrow(arts) / length(news_corpus)
P_B * P_A__B
sum(dfm_select(arts, "artist"))
sum(dfm_select(arts, "artist"))
sum(ntoken(arts))
P_A__B <- sum(dfm_select(arts, "artist")>0) / nrow(arts)
P_B * P_A__B
P_A__B <- sum(dfm_select(arts, "artist")) / sum(ntoken(arts))
P_B <- nrow(arts) / length(news_corpus)
P_B * P_A__B
## reproduce our corpus with all headline classes
news_corpus <- news_data |>
corpus(text_field = "headline")
set.seed(123) # reproducible random number generation
train.prop <- 0.8
## assign corpus IDs to training or test set
ids <- (1:length(news_corpus))
train.ids <- sample(ids, size = ceiling(train.prop * length(ids)), replace=F)
test.ids <- ids[-train.ids]
train.set <- news_corpus[train.ids]
test.set <- news_corpus[test.ids]
train.dfm <- train.set |>
tokens(remove_punct = T) |> # also remove punctuation
dfm() |>
dfm_wordstem() |>
dfm_remove(stopwords("en"))
test.dfm <- test.set |>
tokens(remove_punct = T) |>
dfm() |>
dfm_wordstem() |>
dfm_remove(stopwords("en"))
test.dfm <- dfm_match(test.dfm, features = featnames(train.dfm))
all.equal(featnames(train.dfm), featnames(test.dfm))
nb_model <- textmodel_nb(x = train.dfm,
y = train.set$category,
smooth = 0, # no smoothing
prior = "uniform" # what does this mean?
)
predictions <- predict(nb_model, newdata = test.dfm)
## we should do at least this well
baseline <- max(prop.table(table(test.set$category)))
table(test.set$category)
prop.table(table(test.set$category))
cmat <- table(test.set$category, predictions)
## get confusion matrix
cmat <- table(test.set$category, predictions)
cmat[1:5, 1:5]
nb_acc <- sum(diag(cmat))/sum(cmat)
## how did we do predicting ARTS & CULTURE?
nb_recall <- cmat[2,2]/sum(cmat[2,]) # recall = TP / (TP + FN)
nb_precision <- cmat[2,2]/sum(cmat[,2]) # precision = TP / (TP + FP)
nb_f1 <- 2*(nb_recall*nb_precision)/(nb_recall + nb_precision)
cat(
"Baseline Accuracy: ", baseline, "\n",
"Accuracy:",  nb_acc, "\n",
"Recall:",  nb_recall, "\n",
"Precision:",  nb_precision, "\n",
"F1-score:", nb_f1
)
prop.table(table(test.set$category))
nb_acc
cons_labour_df <- read.csv("data/cons_labour_manifestos.csv", stringsAsFactors = F)
View(cons_labour_df)
## only keep the text and the dependent variable class
cons_labour_df <- cons_labour_df[, c("text", "party")]
## assign 80% to training, 20 to testing
train.prop <- 0.8
## draw proportionate random samples
ids <- (1:nrow(cons_labour_df))
train.ids <- sample(ids, size = ceiling(train.prop * length(ids)), replace=F)
test.ids <- ids[-train.ids]
train.set <- cons_labour_df[train.ids, ]
test.set <- cons_labour_df[test.ids, ]
## Step 2: convert to dfm
## also stem, and remove stopwords
train.dfm <- train.set |>
corpus() |>
tokens(remove_punct = T) |> # also remove punctuation
dfm() |>
dfm_wordstem() |>
dfm_remove(stopwords("en"))
test.dfm <- test.set |>
corpus() |>
tokens(remove_punct = T) |>
dfm() |>
dfm_wordstem() |>
dfm_remove(stopwords("en"))
train.set$party
(train.set$party == "Lab")
outcome <- (2 * (train.set$party == "Lab")) - 1
outcome
## Step 4: fit the wordscore model
ws_base <- textmodel_wordscores(train.dfm, y = outcome)
summary(ws_base)
coef(ws_base)
lab_features <- sort(ws_base$wordscores, decreasing = TRUE)  # for labor
lab_features[1:10]
con_features <- sort(ws_base$wordscores, decreasing = FALSE)  # for conservative
con_features[1:10]
ws_base$wordscores[c("drug", "minor", "unemploy")]
## Step 5: predict values for test set speeches
test.set$party
(pred_ws <- predict(ws_base, newdata = test.dfm,
rescaling = "none", level = 0.95, se.fit = T))
## plot the predicted values for the test texts
textplot_scale1d(pred_ws)
hist(pred_ws$fit, xlim = c(-1, 1), main = "Text Partisanship Score")
sotu_speeches <- data_corpus_sotu |>
corpus_subset(party %in% c("Democratic", "Republican"))
## use US SOTU speeches to build the Wordscore model
sotu_speeches <- data_corpus_sotu |>
corpus_subset(party %in% c("Democratic", "Republican"))
sotu.dfm <- sotu_speeches |>
corpus() |>
tokens(remove_punct = T) |>
dfm() |>
dfm_wordstem() |>
dfm_remove(stopwords("en"))
outcome.sotu <- (2 * (sotu_speeches$party == "Democratic")) - 1
## fit the model to SOTU
ws_base <- textmodel_wordscores(sotu.dfm, y = outcome.sotu)
pred_sotutrain <- predict(ws_base, newdata = test.dfm,
rescaling = "none", level = 0.95, se.fit = T)
ground.truth <- (2 * (test.set$party == "Lab")) - 1
modelA <- cor(pred_ws$fit, ground.truth)
modelB <- cor(pred_sotutrain$fit, ground.truth)
cat(sprintf("OOS corr. for domain-specific: %s \nOOS corr. for off-the-shelf: %s",
round(modelA, 2), round(modelB, 2)))
