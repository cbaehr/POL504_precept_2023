o22 <- N - o21 - o11 - o12
# contingency table
out <- matrix(c(o11, o12, o21, o22),
ncol = 2,
byrow = T)
rownames(out) <- c("United", "Not United")
colnames(out) <- c("Kingdom", "Not Kingdom")
(out)
# expected frequency
E11 <- (o11+o12)/N * (o11 + o21)/N * N
# print
cat("Observed frequency:", o11, "\n\n",
"Expected frequency:", E11)
help("textstat_collocations")
textstat_collocations(mani.collapse)
colloc <- textstat_collocations(mani.collapse)
View(colloc)
colloc[colloc$collocation=="united kingdom" , c("lambda", "z")]
textstat_collocations(mani.collapse, min_count = 5)
colloc <- textstat_collocations(mani.collapse, min_count = 5)
colloc$collocation[order(colloc$lambda)][1:10]
colloc$collocation[order(colloc$count)][1:10]
help(order)
colloc$collocation[order(colloc$lambda, decreasing = T)][1:10]
colloc$collocation[order(colloc$count, decreasing = T)][1:10]
prepare_dt <- function(book_id, removePunct = TRUE){
meta <- gutenberg_works(gutenberg_id  == book_id)
meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
text <- gutenberg_download(book_id) %>%
select(text) %>%
filter(text!="") %>%
unlist() %>%
paste(., collapse = " ") %>%
str_squish(.) %>%
str_trim(., side = "both")
text <- gsub("`|'", "", text) # remove apostrophes
text <- gsub("[^[:alpha:]]", " ", text) # remove all non-alpha characters
output <- tibble(title = meta$title, author = meta$author, text = text)
}
# run function
novels <- lapply(c(64317, 2489), prepare_dt, removePunct = TRUE) %>% do.call(rbind,.)
pacman::p_load(corpus, dplyr, gutenbergr, quanteda, quanteda.corpora, quanteda.textstats,
readtext, rperseus, sotu, stringr, stylest, text.alignment)
prepare_dt <- function(book_id, removePunct = TRUE){
meta <- gutenberg_works(gutenberg_id  == book_id)
meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
text <- gutenberg_download(book_id) %>%
select(text) %>%
filter(text!="") %>%
unlist() %>%
paste(., collapse = " ") %>%
str_squish(.) %>%
str_trim(., side = "both")
text <- gsub("`|'", "", text) # remove apostrophes
text <- gsub("[^[:alpha:]]", " ", text) # remove all non-alpha characters
output <- tibble(title = meta$title, author = meta$author, text = text)
}
# run function
novels <- lapply(c(64317, 2489), prepare_dt, removePunct = TRUE) %>% do.call(rbind,.)
# create dfm
dfm <- tokens(novels$text, remove_punct = T) %>%
dfm(tolower = T)
# regression to check if slope is approx -1.0
regression <- lm(log10(topfeatures(dfm, 100)) ~ log10(1:100))
summary(regression)
confint(regression)
# create plot to illustrate zipf's law
plot(log10(1:100), log10(topfeatures(dfm, 100)),
xlab="log10(rank)", ylab="log10(frequency)", main="Top 100 Words")
# create plot to illustrate zipf's law
plot(log(1:100), log(topfeatures(dfm, 100)),
xlab="log(rank)", ylab="log(frequency)", main="Top 100 Words")
abline(regression, col="red")
abline(a = regression$coefficients["(Intercept)"], b = -1, col = "black")
regression <- lm(log(topfeatures(dfm, 100)) ~ log(1:100))
summary(regression)
confint(regression)
# create plot to illustrate zipf's law
plot(log(1:100), log(topfeatures(dfm, 100)),
xlab="log(rank)", ylab="log(frequency)", main="Top 100 Words")
abline(regression, col="red")
abline(a = regression$coefficients["(Intercept)"], b = -1, col = "black")
num_tokens <- sum(rowSums(dfm))
M <- nfeat(dfm)
k <- 44
# solve for b
b <- log(M/k)/log(num_tokens)
b
corpus <- corpus_subset(data_corpus_ukmanifestos, Year %in% c(1945:1955))
corpus
docvars(corpus)
# key words in context
corpus_subset(corpus, party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
# key words in context
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus <- tolower(corpus)
# key words in context
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus <- corpus_subset(data_corpus_ukmanifestos, Year %in% c(1945:1955))
corpus <- tolower(corpus)
# key words in context
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus <- corpus_subset(data_corpus_ukmanifestos, Year %in% c(1945:1955))
corpus <- tolower(corpus)
# key words in context
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("churchil", window = 5)
topfeatures(corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T))
topfeatures(corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>% dfm())
topfeatures(corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>% tokens_remove(stopwords()) %>% dfm())
topfeatures(corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>% tokens_remove(stopwords()) %>% dfm(), 100)
grep("churchil", corpus)
corpus_subset(corpus, Party == "Con") %>%
tokens(remove_punct = T) %>%
kwic("interest", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("nation", window = 5)
corpus_subset(corpus, Party == "Lib") %>%
tokens(remove_punct = T) %>%
kwic("interest", window = 5)
sotu.sub <- sotu[which(sotu$year %in% 1982:2020),]
sotu.tokens <- tokens(sotu.sub)
sotu.sub <- corpus_reshape(sotu.sub, "sentence")
sotu.tokens <- tokens(sotu.sub)
sotu.sub <- data.frame(corpus_reshape(sotu.sub, "sentence"))
View(sotu.sub)
sotu.sub <- corpus_reshape(sotu.sub, "sentence")
sotu.sub <- sotu[which(sotu$year %in% 1982:2020),]
sotu.sub <- corpus_reshape(sotu.sub, "sentence")
docvars(sotu.sub)
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub["year"])) |>
setNames("text", "year")
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub["year"])) |>
setNames(c("text", "year"))
docvars(sotu.sub["year"])
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"]) |>
setNames(c("text", "year"))
sotu.split <- split(sotu.sub, as.factor(sotu.df$year))
View(sotu.split)
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
View(sotu.split)
docvars(sotu.sub)
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"]) |>
setNames(c("text", "year", "sotu_type"))
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"], docvars(sotu.sub)["sotu_type"]) |>
setNames(c("text", "year", "type"))
sotu.split <- split(sotu.sub, as.factor(sotu.df$year))
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
View(sotu.split)
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"], docvars(sotu.sub)["sotu_type"]) |>
setNames(c("text", "year", "type"))
sotu.split <- split(sotu.df, as.factor(sotu.df$year))
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"]) |>
setNames(c("text", "year"))
sotu.split <- split(sotu.df, as.factor(sotu.df$year))
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
docs.boot <- corpus(year[docnums, "text"])
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- corpus(year[docnums, "text"])
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
iter <- 10 # NUMBER OF BOOTSTRAP SAMPLES (usually would want more, >=100)
## for loop to compute as many samples as specified
for(i in 1:iter) {
if(i==1) {boot.means <- list()} # generate new list
# store the results in new element i
boot.means[[i]] <- lapply(budget.lp.df.SPLIT, boot.fre)
print(i)
}
iter <- 10 # NUMBER OF BOOTSTRAP SAMPLES (usually would want more, >=100)
## for loop to compute as many samples as specified
for(i in 1:iter) {
if(i==1) {boot.means <- list()} # generate new list
# store the results in new element i
boot.means[[i]] <- lapply(sotu.split, boot.fre)
print(i)
}
## combine the point estimates to a data frame and compute statistics by party
boot.means.df <- do.call(rbind.data.frame, boot.means)
mean.boot <- apply(boot.means.df, 2, mean)
sd.boot <- apply(boot.means.df, 2, sd)
## create data frame for plot
plot_df <- data.frame(sotu.df$year, mean.boot, sd.boot) |>
setNames(c("year", "mean", "se"))
sotu.df$year
mean.boot
## create data frame for plot
plot_df <- data.frame(sort(unique(sotu.df$year)), mean.boot, sd.boot) |>
setNames(c("year", "mean", "se"))
## confidence intervals
ci90 <- qnorm(0.95)
ci95 <- qnorm(0.975)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
geom_linerange(aes(x = year,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = year,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
theme(legend.position = "none") # fancy stuff
pacman::p_load(corpus, dplyr, ggplot2, gutenbergr, quanteda, quanteda.corpora, quanteda.textstats,
readtext, rperseus, sotu, stringr, stylest, text.alignment)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
geom_linerange(aes(x = year,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = year,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
theme(legend.position = "none") # fancy stuff
flesch_point <- sotu.df$text %>%
textstat_readability(measure = "Flesch") %>%
group_by(sotu.df$year) %>%
summarise(mean_flesch = mean(Flesch)) %>%
setNames(c("year", "mean")) %>%
arrange(as.numeric(year))
cbind(flesch_point, "bs_mean" = plot_df$mean)
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "Dale.Chall"))
# compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$Dale.Chall))
# print
rownames(readability_cor) <- c("Flesch", "Dale-Chall")
colnames(readability_cor) <- c("Flesch", "Dale-Chall")
readability_cor
docs <- corpus( readtext(paste0("/Users/christianbaehr/Documents/GitHub/pol504_private/homework1/", c("melania", "michelle"), ".txt")) )
# set gap to default (-1)
sw2 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words", gap=-1)
# increase gap penalty to -5 --> reduces extent of plagiarism. Why?
sw3 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words", gap=-5)
sw2
sw3
View(sw2)
sw2$sw
sw3$sw
help("textstat_readability")
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "FOG"))
View(fre_and_dc_measures)
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "FOG"))
# compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$FOG))
# print
rownames(readability_cor) <- c("Flesch", "FOG")
colnames(readability_cor) <- c("Flesch", "FOG")
readability_cor
rm(list = ls())
setwd("/Users/christianbaehr/Documents/GitHub/pol504_private/homework1/")
pacman::p_load(corpus, dplyr, ggplot2, gutenbergr, quanteda, quanteda.corpora, quanteda.textstats,
readtext, rperseus, sotu, stringr, stylest, text.alignment)
R.Version()
library(sf)
## working directory
setwd("/Users/christianbaehr/Documents/GitHub/POL504_precept_2023/")
pacman::p_load(corpus, dplyr, gutenbergr, quanteda, quanteda.corpora,
quanteda.dictionaries, readtext, stringr, stylest)
gutenberg_works()
books <- gutenberg_works() |>
filter(author=="Austen, Jane")
View(books)
View(books)
pride <- gutenberg_download(gutenberg_id = 1342)
## the stylest package comes with some example data
data("novels_excerpts")
View(novels_excerpts)
## who are the authors?
unique(novels_excerpts$author)
filter <- text_filter(drop_punct = TRUE, drop_number = TRUE)  # pre-processing choices
set.seed(123)
seq(10, 90, 10)
mod.cv <- stylest_select_vocab(novels_excerpts$text,
speaker = novels_excerpts$author,
filter = filter,
smooth = 0.5,
nfold = 10,
cutoff_pcts = seq(10, 90, 10))
mod.cv$cutoff_pct_best # cutoff percentile with best performance
mod.cv$miss_pct
mod.cv$miss_pct
apply(mod.cv$miss_pct, 2, mean) # average miss pct. by percentile
mod.cv$cutoff_pct_best
mod.terms <- stylest_terms(novels_excerpts$text,
speaker = novels_excerpts$author,
vocab_cutoff = mod.cv$cutoff_pct_best,
filter = filter)
mod.terms
mod.fit <- stylest_fit(novels_excerpts$text,
speaker = novels_excerpts$author,
terms = mod.terms,
filter = filter)
## explore output
View(mod.fit)
term_usage <- mod.fit$rate
term_usage
authors <- unique(novels_excerpts$author)
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) |>
setNames(authors)
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) |>
setNames(authors)
head(stylest_term_influence(mod.fit,
novels_excerpts$text,
novels_excerpts$author))
View(pride)
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 100)] %>% # randomly select 100
paste(collapse = " ") # collapse to single string
pred <- stylest_predict(mod.fit, pred.text)
pred$predicted # predicted author
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] %>% # randomly select 100
paste(collapse = " ") # collapse to single string
pred <- stylest_predict(mod.fit, pred.text)
pred$predicted # predicted author
pred$log_probs
exp(-1529.687)
grep("c", c("Get", "to", "da", "choppa!"), value=T)
string <- "Its October fifth 2023, and the weather is sunny and 72 degrees."
## 2)
string <- "Its October fifth 2023, and the weather is sunny and 72 degrees." # string
matches <- gregexpr("\\d{2,4}", string) # match positions
matches
regmatches(string, matches)
## 2)
string <- "Its October 5th 2023, and the weather is sunny and 72 degrees." # string
matches <- gregexpr("\\d{2,4}", string) # match positions
regmatches(string, matches) # text
## lets focus on the first task
words <- c("Washington Post", "NYT", "Wall Street Journal", "Peer-2-Peer", "Red State", "Cheese", "222", ",")
words
grep("\\w", words, value = T) # any words
grep("\\w{7}", words, value = T) # at LEAST seven words
grep("\\d", words, value = T) # any numbers
grep("\\W", words, value = T) # any NON-word characters
presidents <- c("Roosevelt-33", "Roosevelt-37", "Obama-2003")
## Use gsub to replace patterns with a string or stringr's str_replace(_all) and str_sub
## Parentheses can identify groups that can later be referenced by \\1 - \\2
gsub("(\\w+)-(\\d{2})", "\\1-19\\2", presidents)
gsub("(\\w+)-(\\d{2})$", "\\1-19\\2", presidents)
x <- "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, nine Minor Outlying Islands, and 326 Indian reservations."
str_view_all(tolower(x), "united(?= states)") # lookahead
str_view_all(tolower(x), "(?<!united )states") # negative lookbehind
data("data_corpus_irishbudgets")
irishbudgets_dfm <- tokens(data_corpus_irishbudgets) %>%
dfm() %>%
dfm_select(pattern = c("tax|budg|auster"),
valuetype = "regex")
featnames(irishbudgets_dfm)
manifestos <- read.csv("data/conservative_manifestos.csv", stringsAsFactors = F)
View(manifestos)
## the Laver Garry dictionary was developed to estimate policy positions of UK
## lawmakers. The dictionary has 7 policy levels, and 19 sub-categories
lgdict <- data_dictionary_LaverGarry
manifestos_lg <- manifestos$text |>
tokens() |>
dfm() |>
dfm_lookup(lgdict)
## what are these policy levels?
View(lgdict)
# how does this look
as.matrix(manifestos_lg)[1:5, 1:5]
# how does this look
as.matrix(manifestos_lg)[1:5, 1:5]
featnames(manifestos_lg)
manifestos$year
plot(manifestos$year,
manifestos_lg[,"CULTURE.SPORT"],
xlab="Year", ylab="SPORTS", type="b", pch=19)
## plot conservative values trend
plot(manifestos$year,
manifestos_lg[,"VALUES.CONSERVATIVE"],
xlab="Year", ylab="Conservative values", type="b", pch=19)
#install.packages("SentimentAnalysis)
harvard <- SentimentAnalysis::DictionaryGI |>
dictionary()
sotu <- data_corpus_sotu |>
tokens() |>
dfm() |>
dfm_lookup(harvard)
## what are the dimensions?
featnames(sotu)
View(harvard)
year <- docvars(data_corpus_sotu)["Date"][,1] |>
as.character() |>
substr(1, 4) |>
as.numeric()
plot(year,
sotu[,"positive"] - sotu[,"negative"],
xlab="Year", ylab="Net Positivity", type="b", pch=19)
help
help(install.packages)
library(rtools)
library(Rtools)
install.packages("Rtools")
install.packages("ttools")
install.packages("rtools")
tmp=tempfile()
list.files(tmp)
help(system2)
