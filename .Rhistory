library(ggplot2)
library(hrbrthemes)
library(RColorBrewer)
p <- ggplot(data = dat, aes(x=Year, after_stat(count), fill=Purpose)) +
geom_density(position = "stack") +
theme_bw() +
theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "black")) +
labs(y="Count") +
scale_fill_manual(values=cbPalette)
p
ggsave("/Users/christianbaehr/Dropbox/satellite_paper/descriptive/satellite_papers_image_purpose.png",
device = "png", width=7, height=4, units = "in")
# p <- ggplot(data = dat, aes(x=Year, after_stat(count), fill=Subfield)) +
#   geom_density(position = "stack")
# p
p <- ggplot(data = dat, aes(x=Year, after_stat(count), fill=Measure)) +
geom_density(position = "stack") +
theme_bw() +
theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "black")) +
labs(y="Count") +
scale_fill_manual(values=cbPalette)
p
ggsave("/Users/christianbaehr/Dropbox/satellite_paper/descriptive/satellite_papers_image_measure.png",
device = "png", width=7, height=4, units = "in")
install.packages("redist")
library(redist)
data("fl250")
class(data)
class(fl250)
library(sf)
write_sf(fl250, "/Users/christianbaehr/Desktop/fl250.geojson")
rm(fl250)
library(sf)
test=st_read("/Users/christianbaehr/Downloads/dataverse_files-11/amos_mcdonald_watkins/amos_mcdonald_watkins.shp")
summary(test$precinctid)
## Lab adapted from: Elisa Wirsching, Lucia Motolinia, Pedro L. Rodriguez, Kevin
## Munger, Patrick Chester and Leslie Huang.
## This precept is designed to help you implement the theory you learn in
## lectures. I will be delivering the content using raw R code so that you see
## how to implement these concepts directly, avoiding the laborious step of
## transferring what you learn from slides or documents into R.
################################################# Precept 2: What is R?
## 1.1) working directory
## point this to directory containing precept files
setwd("/Users/christianbaehr/Documents/GitHub/POL504_precept_2023/")
## 1.2) package management
## only do these once
#install.packages("pacman")
#devtools::install_github("matthewjdenny/preText")
pacman::p_load(quanteda, readtext, preText) #instead of loading individual libs
## 1.3) load movie reviews into a corpus
## load csv with text in "review" column
reviews.raw <- readtext("data/reviews.csv", text_field = "review")
reviews <- corpus(reviews.raw) # convert to corpus form
## what constitutes a document in this corpus?
## what if we want our documents to be sentences?
## convert to sentence level corpus
reviews.sentence <- corpus_reshape(reviews, to = "sentences")
## lets build a document feature matrix
reviews.dfm <- tokens(reviews,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T) |> # tokenize, remove punctuation
# symbols numbers and urls
tokens_remove(stopwords("en")) |> # remove stopwords
tokens_remove("br") |> # remove "br"
tokens_wordstem() |> # stem
dfm()
help("stopwords") # quanteda has stopwords for MANY languages
topfeatures(reviews.dfm)
data_corpus_sotu
meta(reviews)
docvars(reviews)
token_plot <- ggplot(data = reviews,
aes(x = Date,
y = Tokens,
group = 1)) +
geom_line() +
geom_point() +
theme_bw()
library(ggplot2)
token_plot <- ggplot(data = reviews,
aes(x = Date,
y = Tokens,
group = 1)) +
geom_line() +
geom_point() +
theme_bw()
summary(reviews)
ndoc(reviews)
info <- summary(reviews, n=ndoc)
info <- summary(reviews, n=ndoc(reviews))
info
token_plot <- ggplot(data = reviews,
aes(x = Sentences,
group = 1)) +
geom_histogram()
info <- summary(reviews, n=ndoc(reviews))
token_plot <- ggplot(data = info,
aes(x = Sentences,
group = 1)) +
geom_histogram()
token_plot
token_plot <- ggplot(data = info,
aes(x = Sentences,
group = 1)) +
geom_histogram() +
theme_bw()
token_plot
token_plot <- ggplot(data = info,
aes(x = Sentences,
group = 1)) +
geom_histogram()
token_plot
token_plot <- ggplot(data = info,
aes(x = Sentences)) +
geom_histogram()
token_plot
token_plot <- ggplot(data = info,
aes(x = Sentences)) +
geom_histogram()
token_plot
info <- summary(reviews, n=ndoc(reviews))
info <- summary(reviews, n=ndoc(reviews)) # document info
View(info)
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review")
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
info <- summary(reviews, n=ndoc(reviews)) # document info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
token_plot <- ggplot(data = info,
aes(x = Tokens)) +
geom_histogram() # distribution of the number of sentences
token_plot
## retrieve document level info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
help(tokens_wordstem)
char_wordstem(c("win", "winning", "wins", "won", "winner"))
## (we could convert to sentence level corpus)
corpus_reshape(reviews, to = "sentences")
reviews.dfm <- tokens(reviews,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T) |> # tokenize, remove punctuation/symbols/numbers/urls
tokens_remove(stopwords("en")) |> # remove stopwords
tokens_remove("br") |> # remove "br" (Gucci Mane is not a movie reviewer)
tokens_wordstem() |> # use the quanteda stemmer
dfm()
help("stopwords") # quanteda has stopwords for MANY languages
help("stopwords::stopwords") # quanteda has stopwords for MANY languages
help("stopwords") # quanteda has stopwords for MANY languages
topfeatures(reviews.dfm)
textplot_wordcloud(reviews.dfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
## point this to directory containing precept files
setwd("/Users/christianbaehr/Documents/GitHub/POL504_precept_2023/")
pacman::p_load(readtext, preText, quanteda.textplots) #instead of loading individual libs
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
rm(list = ls())
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
## retrieve document level info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
sentence_plot <- ggplot(data = reviews.info, aes(x = Sentences)) +
geom_histogram() # distribution of the number of sentences
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
readtext,
preText,
quanteda.textplots)
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
## retrieve document level info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
sentence_plot <- ggplot(data = reviews.info, aes(x = Sentences)) +
geom_histogram() # distribution of the number of sentences
sentence_plot
## (we could convert to sentence level corpus)
corpus_reshape(reviews, to = "sentences")
## lets build a document feature matrix
reviews.dfm <- tokens(reviews,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T) |> # tokenize, remove punctuation/symbols/numbers/urls
tokens_remove(stopwords("en")) |> # remove stopwords
tokens_remove("br") |> # remove "br" (Gucci Mane is not a movie reviewer)
tokens_wordstem() |> # use the quanteda stemmer
dfm()
help("stopwords") # quanteda has stopwords for MANY languages
topfeatures(reviews.dfm)
textplot_wordcloud(reviews.dfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm) # defaults to "absolute frequency"
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop") # defaults to "absolute frequency"
topfeatures(reviews.dfm_prop)
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),]) # why is ordering identical as frequency weighting?
topfeatures(reviews.dfm_weighted)
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm) # defaults to "absolute frequency"
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm) # defaults to "absolute frequency"
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop") # relative frequency
reviews.dfm[1]
reviews.dfm[1,]
reviews.dfm_weighted[1,]
reviews.dfm_weighted[,1]
reviews.dfm_weighted[,1] > 0
which(reviews.dfm_weighted[,1] > 0)
reviews.dfm_weighted[,1]@x
1/3
1/(3/100)
reviews.dfm[,1]@x
ndoc(reviews.dfm[,1])
reviews.dfm[1,]@x
sum(reviews.dfm[1,]@x)
1/222
num=1/222
denom=log(100/3)
num*denom
num/denom
reviews.dfm_weighted
reviews.dfm_weighted[1,1]
denom=100/3
num*denom
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm) # defaults to "absolute frequency"
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm) # defaults to "absolute frequency"
help(dfm_tfidf)
num=1
num*denom
reviews.dfm[1,]
reviews.dfm[,1]
reviews.dfm[,1]@x
3/100
100/3
denom=log(100/3)
denom
num*denom
num=1
num*denom
help(dfm_tfidf)
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, scheme_df = "count") # defaults to "absolute frequency"
reviews.dfm_weighted
1*3
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm) # defaults to "absolute frequency"
reviews.dfm_weighted
log(3)
log(100/3)
log(3/100)
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop") # relative frequency
reviews.dfm_prop
tf=1/reviews.dfm[1,]
tf
tf=1/sum(reviews.dfm[1,]@x)
tf
idf=log(100/(length(reviews.dfm[,1]@x)))
idf
tf*idf
idf=100/(length(reviews.dfm[,1]@x))
idf
tf*idf
topfeatures(reviews.dfm_prop)
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),]) # why is ordering identical as frequency weighting?
topfeatures(reviews.dfm_weighted)
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
View(reviews.dfm)
non_dfm <- convert(reviews.dfm)
View(non_dfm)
mat = non_dfm[[1]]
View(mat)
mat = data.frame(non_dfm[[1]])
View(mat)
mat = non_dfm[[1]]
View(mat)
dfm_tfidf()
dfm_tfidf
help(ln)
ln
log
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # defaults to "absolute frequency"
reviews.dfm_weighted
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm) # defaults to "absolute frequency"
reviews.dfm_weighted
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # defaults to "absolute frequency"
tf
idf
reviews.dfm_weighted
tf*idf
tf=1/sum(reviews.dfm[1,])
idf=log(100/length(reviews.dfm[,1]@x))
exp(1)
e
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop",
base = exp(1)) # relative frequency
reviews.dfm_prop
tf*idf
topfeatures(reviews.dfm_prop)
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),])
topfeatures(reviews.dfm_weighted)
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # defaults to "absolute frequency"
## ALSO DEFAULTS TO LOG_10 !!!
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop",
base = exp(1)) # relative frequency
topfeatures(reviews.dfm_prop)
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),])
topfeatures(reviews.dfm_weighted)
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm) # defaults to "absolute frequency"
## ALSO DEFAULTS TO LOG_10 !!!
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop") # relative frequency
topfeatures(reviews.dfm_prop)
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),])
topfeatures(reviews.dfm_weighted)
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
## Introduction to R Programming
## Date: September 21, 2023
## Author: Christian Baehr
## Lab adapted from: Elisa Wirsching, Lucia Motolinia, Pedro L. Rodriguez, Kevin
## Munger, Patrick Chester and Leslie Huang.
## This precept is designed to help you implement the theory you learn in
## lectures. I will be delivering the content using raw R code so that you see
## how to implement these concepts directly, avoiding the laborious step of
## transferring what you learn from slides or documents into R.
################################################# Precept 2: What is R?
## 1.1) working directory
## point this to directory containing precept files
setwd("/Users/christianbaehr/Documents/GitHub/POL504_precept_2023/")
## 1.2) package management
## only do these once
#install.packages("pacman")
#devtools::install_github("matthewjdenny/preText")
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
readtext,
preText,
quanteda.textplots)
## 1.3) load movie reviews into a corpus
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
## what constitutes a DOCUMENT in this corpus?
## retrieve document level info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
sentence_plot <- ggplot(data = reviews.info, aes(x = Sentences)) +
geom_histogram() # distribution of the number of sentences
sentence_plot
## (we could convert to sentence level corpus)
corpus_reshape(reviews, to = "sentences")
## lets build a document feature matrix
reviews.dfm <- tokens(reviews,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T) |> # tokenize, remove punctuation/symbols/numbers/urls
tokens_remove(stopwords("en")) |> # remove stopwords
tokens_remove("br") |> # remove "br" (Gucci Mane is not a movie reviewer)
tokens_wordstem() |> # use the quanteda stemmer
dfm()
help("stopwords") # quanteda has stopwords for MANY languages
topfeatures(reviews.dfm)
textplot_wordcloud(reviews.dfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
## weighted dfms
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # defaults to "absolute frequency"
## ALSO DEFAULTS TO LOG_10 !!!
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop",
base = exp(1)) # relative frequency
topfeatures(reviews.dfm_prop)
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),])
topfeatures(reviews.dfm_weighted)
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),])
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
tokens(reviews.dfm)
kwic(tokens(reviews))
tokens(reviews)
kwic(tokens(reviews), pattern = "love", valuetype = "regex", window = 4)
kwic(tokens(reviews), pattern = "love", valuetype = "regex", window = 4)
kwic(tokens(reviews), pattern = "hate", valuetype = "regex", window = 4)
kwic_love <- kwic(tokens(reviews), pattern = "love", valuetype = "regex", window = 4)
kwic_hate <- kwic(tokens(reviews), pattern = "hate", valuetype = "regex", window = 4)
head(kwic_love)
head(kwic_hate)
View(kwic_hate)
help(kwic)
kwic_love <- kwic(tokens(reviews),
pattern = "love",
valuetype = "exact",
window = 4)
kwic_love <- kwic(tokens(reviews),
pattern = "love",
valuetype = "fixed",
window = 4)
View(kwic_love)
help(textstat_collocations)
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
readtext,
preText,
quanteda.textplots,
quanteda.textstats)
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
preText,
quanteda.textplots,
quanteda.textstats,
readtext)
textstat_collocations(reviews)
reviews.colloc <- textstat_collocations(reviews)
View(reviews.colloc)
reviews.colloc.3 <- textstat_collocations(reviews) # default is bigrams
reviews.colloc.3 <- textstat_collocations(reviews, size = 3)
View(reviews.colloc.3)
reviews.colloc.2_4 <- textstat_collocations(reviews, size = 2:4)
View(reviews.colloc.2_4)
see("abc ABC 123\t.!?\\(){}\n", "a")
see <- function(string, rx) stringr::str_view_all(string, rx)
see("abc ABC 123\t.!?\\(){}\n", "a")
see("abc ABC 123\t.!?\\(){}\n", "\\s")
grepl("^so", c("so today we", "never so today", "today never so"))
mysentence <- "We've never lost an American in space, we're sure as hell not gonna lose one on my watch! Failure is not an option."
mysentence <- strsplit(mysentence, split = " ")[[1]]
mysentence
mysentence <- grep("los", mysentence, value = T)
mysentence <- "We've never lost an American in space, we're sure as hell not gonna lose one on my watch! Failure is not an option."
mysentence <- strsplit(mysentence, split = " ")[[1]]
grep("los", mysentence, value = T)
grep("a", mysentence, value = T)
grep("^a", mysentence, value = T)
grep("a", mysentence, value = T) # any element with "a"
grep("^a", mysentence, value = T) # any element STARTING with "a"
grep("[^\w\s]+", mysentence, value = T)
"[^\w\s]+"
grep("[[:punct:]]", mysentence, value = T)
grep("^F+[a-z0-9.-]", mysentence, value = T)
grep("^F+[a-z0-9.-]+ure$", mysentence, value = T)
grep("^F+[a-z0-9]+ure$", mysentence, value = T)
grep("^F+[A-Z0-9]+ure$", mysentence, value = T)
grep("^F+[a-z0-9]+ure$", mysentence, value = T)
grep("F+a", mysentence)
preprocessed_documents <- factorial_preprocessing(
reviews,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2,
verbose = TRUE)
grep("^Fail?", "FailFail", value = T)
## starts with F, contains middle characters with a-z and/or 0-9 and ends with "ure"
grep("^F[a-z0-9]+ure$", mysentence, value = T)
## starts with F, contains middle characters with a-z and/or 0-9 and ends with "ure"
grep("^F[a-z0-9]ure$", mysentence, value = T)
grep("^F[a-z0-9]&ure$", mysentence, value = T)
grep("F+[a-z0-9]+ure", "ureFai", value = T)
grep("F+[a-z0-9]+ure", "Faure", value = T)
grep("F+[a-z0-9]+ure", "Fure", value = T)
grep("F+[a-z0-9]+ure", "Fakjngaonrgaernure", value = T)
grep("^F{1}[a-z0-9]+ure$", mysentence, value = T)
grep("^F{1}[a-z0-9]{1}ure$", mysentence, value = T)
grep("^F{1}[a-z0-9]{3}ure$", mysentence, value = T)
grep("^F{1}[a-z0-9]?ure$", mysentence, value = T) # why does this work?
grep("^F{1}[a-z0-9]{3}ure$", mysentence, value = T) # why does this work?
grep("^F{1}[a-z0-9]{3}ure$", "FFailure", value = T)
grep("^F+[a-z0-9]{3}ure$", "FFailure", value = T)
grep("^F+[a-z0-9]{3}ure$", "FFailuure", value = T)
preprocessed_documents <- factorial_preprocessing(
reviews,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2,
verbose = TRUE)
preprocessed_documents <- factorial_preprocessing(
reviews,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2,
verbose = TRUE)
head(preprocessed_documents$choices)
preText_results <- preText(preprocessed_documents,
dataset_name = "SOTU Speeches",
distance_method = "cosine",
num_comparisons = 20,
verbose = TRUE)
preText_score_plot(preText_results)
help("preText")
# 2.1 Example using data from the corpus of inaugural speeches
tokens <- tokens(data_corpus_inaugural, remove_punct = TRUE)
Tee <- sum(lengths(tokens))
inaug_dfm <- dfm(tokens)
lengths(tokens)
inaug_dfm <- dfm(tokens)
M <- nfeat(inaug_dfm)  # number of features = number of types
k <- 44
b <- .49
k * (Tee)^b
M
k <- 41
b <- 0.46
k * (Tee)^b
