## for loop to compute as many samples as specified
for(i in 1:iter) {
if(i==1) {boot.means <- list()} # generate new list
# store the results in new element i
boot.means[[i]] <- lapply(sotu.split, boot.fre)
print(i)
}
## combine the point estimates to a data frame and compute statistics by party
boot.means.df <- do.call(rbind.data.frame, boot.means)
mean.boot <- apply(boot.means.df, 2, mean)
sd.boot <- apply(boot.means.df, 2, sd)
## create data frame for plot
plot_df <- data.frame(sotu.df$year, mean.boot, sd.boot) |>
setNames(c("year", "mean", "se"))
sotu.df$year
mean.boot
## create data frame for plot
plot_df <- data.frame(sort(unique(sotu.df$year)), mean.boot, sd.boot) |>
setNames(c("year", "mean", "se"))
## confidence intervals
ci90 <- qnorm(0.95)
ci95 <- qnorm(0.975)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
geom_linerange(aes(x = year,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = year,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
theme(legend.position = "none") # fancy stuff
pacman::p_load(corpus, dplyr, ggplot2, gutenbergr, quanteda, quanteda.corpora, quanteda.textstats,
readtext, rperseus, sotu, stringr, stylest, text.alignment)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
geom_linerange(aes(x = year,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = year,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
theme(legend.position = "none") # fancy stuff
flesch_point <- sotu.df$text %>%
textstat_readability(measure = "Flesch") %>%
group_by(sotu.df$year) %>%
summarise(mean_flesch = mean(Flesch)) %>%
setNames(c("year", "mean")) %>%
arrange(as.numeric(year))
cbind(flesch_point, "bs_mean" = plot_df$mean)
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "Dale.Chall"))
# compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$Dale.Chall))
# print
rownames(readability_cor) <- c("Flesch", "Dale-Chall")
colnames(readability_cor) <- c("Flesch", "Dale-Chall")
readability_cor
docs <- corpus( readtext(paste0("/Users/christianbaehr/Documents/GitHub/pol504_private/homework1/", c("melania", "michelle"), ".txt")) )
# set gap to default (-1)
sw2 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words", gap=-1)
# increase gap penalty to -5 --> reduces extent of plagiarism. Why?
sw3 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words", gap=-5)
sw2
sw3
View(sw2)
sw2$sw
sw3$sw
help("textstat_readability")
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "FOG"))
View(fre_and_dc_measures)
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "FOG"))
# compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$FOG))
# print
rownames(readability_cor) <- c("Flesch", "FOG")
colnames(readability_cor) <- c("Flesch", "FOG")
readability_cor
rm(list = ls())
setwd("/Users/christianbaehr/Documents/GitHub/pol504_private/homework1/")
pacman::p_load(corpus, dplyr, ggplot2, gutenbergr, quanteda, quanteda.corpora, quanteda.textstats,
readtext, rperseus, sotu, stringr, stylest, text.alignment)
R.Version()
library(sf)
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties
gsub("\t", "", counties)
counties <- gsub("\t", "", counties)
gsub("<option value=\", "", counties)
)
sdfa
)))
((()))
(
>
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('<option value=\\\\', "", counties)
counties
gsub('<option value=([\\])', "", counties)
counties <- gsub('<option value=', "", counties)
counties
counties <- gsub('option>', "", counties)
counties
gsub('"', '', counties)
gsub('"|<', '', counties)
gsub('"|<|>', '', counties)
gsub('"|<|>|\\d', '', counties)
gsub('"|<|>|\\d|//', '', counties)
gsub('"|<|>|\\d|////', '', counties)
gsub('"|<|>|\\d|///', '', counties)
gsub('"|<|>|\\d|', '', counties)
counties <- gsub('"|<|>|\\d|', '', counties)
counties
counties <- gsub('////', '', counties)
counties
gsub('////', '', counties)
counties <- gsub("([\\])","", counties)
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|select', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties
gsub("([/])","", counties)
gsub("/","", counties)
counties <- trimws(gsub("/","", counties))
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|select|eded', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties <- trimws(gsub("/","", counties))
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|select|eded', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties <- trimws(gsub("/","", counties))
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|selected', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties <- trimws(gsub("/","", counties))
counties
write.table(counties, "/Users/christianbaehr/Desktop/county_names.txt", row.names = F)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
pacman::p_load(quanteda, quanteda.corpora, quanteda.textstats, dplyr, ggplot2, stringr)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
pacman::p_load(quanteda, quanteda.corpora, quanteda.textstats, dplyr, ggplot2, stringr, sotu)
# merge the meta and text dataframes from the sotu package
sotu <- cbind(sotu_meta, sotu_text)
View(sotu)
sotu <- corpus(sotu, text_field = "sotu.text")
View(sotu)
sotu <- corpus(sotu, text_field = "sotu_text")
# merge the meta and text dataframes from the sotu package
sotu <- cbind(sotu_meta, sotu_text)
sotu <- sotu[which(sotu$year %in% 2007:2010), ]
sotu <- corpus(sotu, text_field = "sotu_text")
sotu.corpus <- corpus(sotu, text_field = "sotu_text")
sotu.tokens <- tokens(sotu.corpus)
library(quanteda.corpora)
ukmnfs <- data_corpus_ukmanifestos
test <- corpus_reshape(ukmnfs, to = "sentences")
library(quanteda)
library(quanteda.corpora)
ukmnfs <- data_corpus_ukmanifestos
test <- corpus_reshape(ukmnfs, to = "sentences")
test.colloc_6b <- textstat_collocations(test) %>% filter(collocation == "united kingdom")
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
pacman::p_load(quanteda, quanteda.corpora, quanteda.textstats, dplyr, ggplot2,
readtext, stringr, sotu, gutenbergr, stylest, text.alignment)
test.colloc_6b <- textstat_collocations(test) %>% filter(collocation == "united kingdom")
library(quanteda.corpora)
ukmnfs <- data_corpus_ukmanifestos
test <- corpus_reshape(ukmnfs, to = "sentences")
test.colloc_6b <- textstat_collocations(test) %>% filter(collocation == "united kingdom")
test.colloc_6b
test.colloc_6c <- textstat_collocations(test,  min_count=5)
##Lambda
test.colloc_6c_top10_lam <- test.colloc_6c %>% arrange(desc(lambda)) %>% top_n(10)
a <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/id.Dennis+Schwartz")
View(a)
a <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/id.Dennis+Schwartz")
b <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/rating.Dennis+Schwartz")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/subj.Dennis+Schwartz")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/subj.Dennis+Schwartz")
help("read.table")
c <- scan("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/subj.Dennis+Schwartz")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/subj.Dennis+Schwartz", sep = "\n")
View(c)
View(c)
a <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/id.James+Berardinelli")
b <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/rating.James+Berardinelli")
b <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/rating.James+Berardinelli")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/subj.James+Berardinelli", sep = "\n")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/subj.James+Berardinelli", sep = "\n", quote = "")
View(c)
reviews <- cbind(a, b, c)
View(reviews)
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
b <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/label.4class.James+Berardinelli")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/subj.James+Berardinelli", sep = "\n", quote = "")
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
View(reviews)
table(reviews$rating)
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/id.%s"
sprintf(root, "James+Berardinelli")
sprintf(root, c("James+Berardinelli", "James+Berardinelli"))
help(sprintf)
sprintf(root, "James+Berardinelli", "James+Berardinelli")
join <- function(name, class) {
a <- read.table(root, class, name, name)
b <- read.table(root, class, name, name)
c <- read.table(root, class, name, name)
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli")
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/%s.%s"
join <- function(name) {
a <- read.table(root, name, "id", name)
b <- read.table(root, name, "label", name)
c <- read.table(root, name, "subj", name)
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli")
lapply(names, join)
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/%s.%s"
join <- function(name) {
a <- read.table(sprintf(root, name, "id", name))
b <- read.table(sprintf(root, name, "label", name))
c <- read.table(sprintf(root, name, "subj", name))
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli")
lapply(names, join)
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/%s.%s"
join <- function(name) {
a <- read.table(sprintf(root, name, "id", name))
b <- read.table(sprintf(root, name, "label.4class", name))
c <- read.table(sprintf(root, name, "subj", name))
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli")
lapply(names, join)
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/%s.%s"
join <- function(name) {
a <- read.table(sprintf(root, name, "id", name))
b <- read.table(sprintf(root, name, "label.4class", name))
c <- read.table(sprintf(root, name, "subj", name), sep = "\n", quote = "")
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli")
lapply(names, join)
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/%s.%s"
join <- function(name) {
a <- read.table(sprintf(root, name, "id", name))
b <- read.table(sprintf(root, name, "label.4class", name))
c <- read.table(sprintf(root, name, "subj", name), sep = "\n", quote = "")
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli", "Dennis+Schwartz", "Scott+Renshaw", "Steve+Rhodes")
out <- lapply(names, join)
out <- do.call(rbind, out)
View(out)
table(out$rating)
write.csv(out, "/Users/christianbaehr/Desktop/movie_reviews.csv", row.names = F)
## load packages
pacman::p_load(quanteda, quanteda.corpora, readtext, quanteda.textmodels,
quanteda.textplots, dplyr)
## working directory
setwd("/Users/christianbaehr/Documents/GitHub/POL504_precept_2023/data/")
## package dependencies
pacman::p_load(ldatuning,
topicmodels,
ggplot2,
dplyr,
rjson,
quanteda,
tidytext,
stringi,
tidyr,
lubridate,
parallel,
doParallel,
stm,
text2vec,
conText)
## Load in Black Lives Matter tweet sample
blm_tweets <- read.csv("blm_samp.csv", stringsAsFactors = F)
View(blm_tweets)
blm_tweets$datetime <- as.POSIXct(strptime(blm_tweets$created_at,
format = "%a %b %d %X %z %Y",
tz = "GMT"))
blm_tweets$date <- mdy(paste(month(blm_tweets$datetime),
day(blm_tweets$datetime),
year(blm_tweets$datetime),
sep = "-"))
blm_tweets_sum <- blm_tweets %>%
group_by(date) %>%
summarise(text = paste(text, collapse = " "))
## Remove non ASCII characters
blm_tweets_sum$text <- stringi::stri_trans_general(blm_tweets_sum$text, "latin-ascii")
## solitary letters
blm_tweets_sum$text <- gsub(" [A-z] ", " ", blm_tweets_sum$text)
blm_dfm <- tokens(blm_tweets_sum$text, remove_punct = T, remove_numbers = T, remove_symbols = T) %>%
dfm(tolower = T) %>%
dfm_remove(c(stopwords("english"), "http","https","rt", "t.co"))
dim(blm_dfm)
help("FindTopicsNumber")
## we will set topics to 5 for times sake
k <- 5
## construct a model in which five topics generated the data
blm_tm <- LDA(blm_dfm, k = k, method = "Gibbs", control = list(seed = 1234))
## look at the topic proportions over documents
dim(blm_tm@gamma)
blm_tm@gamma[1:5,1:5]
rowSums(blm_tm@gamma) # why is this?
colMeans(blm_tm@gamma)
dim(blm_dfm)  # how many features do we have?
dim(blm_tm@beta) # term proportion of each topic
rowSums(exp(blm_tm@beta)) # why is THIS?
blm_tm@beta[1:5,1:5]
## convenient little function
blm_topics <- tidy(blm_tm, matrix = "beta")
head(blm_topics)
blm_top_terms <- blm_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(blm_top_terms)
blm_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free")
blm_topics %>%
mutate(topic = paste0("topic", topic)) %>%
filter(topic %in% c("topic1", "topic2")) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1)) %>%
arrange(-abs(log_ratio)) %>%
slice(c(1:10,(nrow(.)-9):nrow(.))) %>%
arrange(-log_ratio) %>%
mutate(term = factor(term, levels = unique(term))) %>%
ggplot(aes(as.factor(term), log_ratio)) +
geom_col(show.legend = FALSE) +
xlab("Terms") + ylab("Log-Ratio") +
coord_flip()
## Store topic proportions as an object
doc_topics <- t(blm_tm@gamma)
## Arrange topics
# Find the top topic per column (day)
max <- apply(doc_topics, 2, which.max)
## Code police shooting events
victim <- c("Freddie Gray", "Sandra Bland")
shootings <- mdy(c("04/12/2015","7/13/2015"))
blm_topics %>% filter(term=="#freddiegray")
blm_topics %>% filter(term=="#sandrabland")
## Combine data
top1 <- data.frame(top_topic = max, date = ymd(blm_tweets_sum$date)) %>%
filter(date < as.Date("2016-01-01"))
ggplot(top2, aes(x=date, y=top_topic, pch="First")) + theme_bw() +
ylab("Topic Number") + ggtitle("BLM-Related Tweets from 2014 to 2016 over Topics") + geom_point() + xlab(NULL) +
geom_vline(xintercept=as.numeric(shootings[1]), color = "blue", linetype=4) + # Freddie Gray (Topic)
geom_vline(xintercept=as.numeric(shootings[2]), color = "black", linetype=4)  + # Sandra Bland
scale_shape_manual(values=c(18, 1), name = "Topic Rank")
## Plot top topic per day
ggplot(top1, aes(x=date, y=top_topic, pch="First")) + theme_bw() +
ylab("Topic Number") + ggtitle("BLM-Related Tweets from 2014 to 2016 over Topics") + geom_point() + xlab(NULL) +
geom_vline(xintercept=as.numeric(shootings[1]), color = "blue", linetype=4) + # Freddie Gray (Topic)
geom_vline(xintercept=as.numeric(shootings[2]), color = "black", linetype=4)  + # Sandra Bland
scale_shape_manual(values=c(18, 1), name = "Topic Rank")
data(poliblog5k)
head(poliblog5k.meta) ## document metadata
head(poliblog5k.voc) ## the total set of terms in the dfm (ordered)
head(poliblog5k.docs) ## the DOCUMENT SPECIFIC "fm"
## estimate a structural topic model with three topics
# help(stm)
blog_stm <- stm(documents=poliblog5k.docs,
vocab=poliblog5k.voc,
K=3,
prevalence = ~rating + s(day), # topics vary by partisanship and time
data = poliblog5k.meta)
plot(blog_stm, type = "labels") # top terms by topic
plot(blog_stm, type = "summary") # topic prevalence
## most distinctive terms for topic 2 vs. 1
plot(blog_stm, type="perspectives", topics = c(1,2))
## Estimate a linear model of the relationship between topic PREVALENCE and ideology + date
prep <- estimateEffect(1:3 ~ rating + s(day) ,
blog_stm,
nsims = 25,
meta = poliblog5k.meta)
## topic dynamics
plot(prep,
"day",
blog_stm,
topics = c(1,2),
method = "continuous",
xaxt = "n",
xlab = "Date")
## topic prevalence by party
plot(prep,
"rating",
model = blog_stm,
method = "difference",
cov.value1 = "Conservative",
cov.value2 = "Liberal")
## sample of Congressional speech records
corp <- cr_sample_corpus
## tokenize speeches
toks <- tokens(corp,
remove_punct=T,
remove_symbols=T,
remove_numbers=T,
remove_separators=T)
## only retain features that occur AT LEAST ten times in the corpus
feats <- dfm(toks, tolower=T, verbose = FALSE) %>%
dfm_trim(min_termfreq = 10) %>%
featnames()
## new tokens object with only those tokens that occur >=10 times.
## padding replaces infrequent tokens with empty("") vector instead of
## removing them entirely. This retains original spacing, which is important
## for embedding windows.
toks_feats <- tokens_select(toks,
feats,
padding = TRUE)
head(toks_feats)
## 3.3) Fit the GloVE model
## we define the context as a window of the 6 TOKENS on either side of a given
## token
WINDOW_SIZE <- 6
## generate a feature co-occurrence matrix
toks_fcm <- fcm(toks_feats,
context = "window", # rather than document
window = WINDOW_SIZE,
count = "frequency", # as opposed to boolean or weighted function of distance
tri = FALSE) # important to set tri = FALSE (retain full matrix)
head(toks_fcm)
## define some hyperparameters for the model
DIM <- 300
ITERS <- 10
## fit the glove model -- identifying words most likely to be paired
glove <- GlobalVectors$new(rank = DIM,
x_max = 10,
learning_rate = 0.05)
wv_main <- glove$fit_transform(toks_fcm,
n_iter = ITERS,
convergence_tol = 1e-3,
n_threads = 2)
## 3.4) Interpreting the output
dim(wv_main)
word_vectors_context <- glove$components
## While both of word-vectors matrices can be used as result it usually better
## (idea from GloVe paper) to average or take a sum of main and context vector:
word_vectors <- wv_main + t(word_vectors_context) # word vectors
## features?
head(rownames(word_vectors))
## Pretrained GLoVE embeddings
## Download this from Dropbox for your homework
pretrained <- readRDS("~/Dropbox/POL504/glove.rds") # GloVe pretrained (https://nlp.stanford.edu/projects/glove/)
dim(pretrained)
## Function to compute nearest neighbors
nearest_neighbors <- function(cue, embeds, N = 5, norm = "l2"){
cos_sim <- sim2(x = embeds, y = embeds[cue, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # cue is always the nearest neighbor hence dropped
}
## e.g.
nearest_neighbors("state", word_vectors, N = 10, norm = "l2")
nearest_neighbors("state", pretrained, N = 10, norm = "l2")
