docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
View(sotu.split)
docvars(sotu.sub)
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"]) |>
setNames(c("text", "year", "sotu_type"))
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"], docvars(sotu.sub)["sotu_type"]) |>
setNames(c("text", "year", "type"))
sotu.split <- split(sotu.sub, as.factor(sotu.df$year))
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
View(sotu.split)
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"], docvars(sotu.sub)["sotu_type"]) |>
setNames(c("text", "year", "type"))
sotu.split <- split(sotu.df, as.factor(sotu.df$year))
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
#sotu.tokens <- tokens(sotu.sub)
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"]) |>
setNames(c("text", "year"))
sotu.split <- split(sotu.df, as.factor(sotu.df$year))
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- year[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
docs.boot <- corpus(year[docnums, "text"])
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- corpus(year[docnums, "text"])
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(sotu.split, boot.fre) # apply to each df of party texts
iter <- 10 # NUMBER OF BOOTSTRAP SAMPLES (usually would want more, >=100)
## for loop to compute as many samples as specified
for(i in 1:iter) {
if(i==1) {boot.means <- list()} # generate new list
# store the results in new element i
boot.means[[i]] <- lapply(budget.lp.df.SPLIT, boot.fre)
print(i)
}
iter <- 10 # NUMBER OF BOOTSTRAP SAMPLES (usually would want more, >=100)
## for loop to compute as many samples as specified
for(i in 1:iter) {
if(i==1) {boot.means <- list()} # generate new list
# store the results in new element i
boot.means[[i]] <- lapply(sotu.split, boot.fre)
print(i)
}
## combine the point estimates to a data frame and compute statistics by party
boot.means.df <- do.call(rbind.data.frame, boot.means)
mean.boot <- apply(boot.means.df, 2, mean)
sd.boot <- apply(boot.means.df, 2, sd)
## create data frame for plot
plot_df <- data.frame(sotu.df$year, mean.boot, sd.boot) |>
setNames(c("year", "mean", "se"))
sotu.df$year
mean.boot
## create data frame for plot
plot_df <- data.frame(sort(unique(sotu.df$year)), mean.boot, sd.boot) |>
setNames(c("year", "mean", "se"))
## confidence intervals
ci90 <- qnorm(0.95)
ci95 <- qnorm(0.975)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
geom_linerange(aes(x = year,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = year,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
theme(legend.position = "none") # fancy stuff
pacman::p_load(corpus, dplyr, ggplot2, gutenbergr, quanteda, quanteda.corpora, quanteda.textstats,
readtext, rperseus, sotu, stringr, stylest, text.alignment)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
geom_linerange(aes(x = year,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = year,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
theme(legend.position = "none") # fancy stuff
flesch_point <- sotu.df$text %>%
textstat_readability(measure = "Flesch") %>%
group_by(sotu.df$year) %>%
summarise(mean_flesch = mean(Flesch)) %>%
setNames(c("year", "mean")) %>%
arrange(as.numeric(year))
cbind(flesch_point, "bs_mean" = plot_df$mean)
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "Dale.Chall"))
# compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$Dale.Chall))
# print
rownames(readability_cor) <- c("Flesch", "Dale-Chall")
colnames(readability_cor) <- c("Flesch", "Dale-Chall")
readability_cor
docs <- corpus( readtext(paste0("/Users/christianbaehr/Documents/GitHub/pol504_private/homework1/", c("melania", "michelle"), ".txt")) )
# set gap to default (-1)
sw2 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words", gap=-1)
# increase gap penalty to -5 --> reduces extent of plagiarism. Why?
sw3 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words", gap=-5)
sw2
sw3
View(sw2)
sw2$sw
sw3$sw
help("textstat_readability")
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "FOG"))
View(fre_and_dc_measures)
# calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(sotu.sub, c("Flesch", "FOG"))
# compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$FOG))
# print
rownames(readability_cor) <- c("Flesch", "FOG")
colnames(readability_cor) <- c("Flesch", "FOG")
readability_cor
rm(list = ls())
setwd("/Users/christianbaehr/Documents/GitHub/pol504_private/homework1/")
pacman::p_load(corpus, dplyr, ggplot2, gutenbergr, quanteda, quanteda.corpora, quanteda.textstats,
readtext, rperseus, sotu, stringr, stylest, text.alignment)
R.Version()
library(sf)
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties
gsub("\t", "", counties)
counties <- gsub("\t", "", counties)
gsub("<option value=\", "", counties)
)
sdfa
)))
((()))
(
>
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('<option value=\\\\', "", counties)
counties
gsub('<option value=([\\])', "", counties)
counties <- gsub('<option value=', "", counties)
counties
counties <- gsub('option>', "", counties)
counties
gsub('"', '', counties)
gsub('"|<', '', counties)
gsub('"|<|>', '', counties)
gsub('"|<|>|\\d', '', counties)
gsub('"|<|>|\\d|//', '', counties)
gsub('"|<|>|\\d|////', '', counties)
gsub('"|<|>|\\d|///', '', counties)
gsub('"|<|>|\\d|', '', counties)
counties <- gsub('"|<|>|\\d|', '', counties)
counties
counties <- gsub('////', '', counties)
counties
gsub('////', '', counties)
counties <- gsub("([\\])","", counties)
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|select', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties
gsub("([/])","", counties)
gsub("/","", counties)
counties <- trimws(gsub("/","", counties))
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|select|eded', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties <- trimws(gsub("/","", counties))
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|select|eded', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties <- trimws(gsub("/","", counties))
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties
counties <- readLines("/Users/christianbaehr/Desktop/counties.txt")
counties <- gsub("\t", "", counties)
counties <- gsub('option|value|selected', "", counties)
counties <- gsub('"|<|>|\\d|=', '', counties)
counties <- trimws(gsub("/","", counties))
counties
write.table(counties, "/Users/christianbaehr/Desktop/county_names.txt", row.names = F)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
pacman::p_load(quanteda, quanteda.corpora, quanteda.textstats, dplyr, ggplot2, stringr)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
pacman::p_load(quanteda, quanteda.corpora, quanteda.textstats, dplyr, ggplot2, stringr, sotu)
# merge the meta and text dataframes from the sotu package
sotu <- cbind(sotu_meta, sotu_text)
View(sotu)
sotu <- corpus(sotu, text_field = "sotu.text")
View(sotu)
sotu <- corpus(sotu, text_field = "sotu_text")
# merge the meta and text dataframes from the sotu package
sotu <- cbind(sotu_meta, sotu_text)
sotu <- sotu[which(sotu$year %in% 2007:2010), ]
sotu <- corpus(sotu, text_field = "sotu_text")
sotu.corpus <- corpus(sotu, text_field = "sotu_text")
sotu.tokens <- tokens(sotu.corpus)
library(quanteda.corpora)
ukmnfs <- data_corpus_ukmanifestos
test <- corpus_reshape(ukmnfs, to = "sentences")
library(quanteda)
library(quanteda.corpora)
ukmnfs <- data_corpus_ukmanifestos
test <- corpus_reshape(ukmnfs, to = "sentences")
test.colloc_6b <- textstat_collocations(test) %>% filter(collocation == "united kingdom")
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
pacman::p_load(quanteda, quanteda.corpora, quanteda.textstats, dplyr, ggplot2,
readtext, stringr, sotu, gutenbergr, stylest, text.alignment)
test.colloc_6b <- textstat_collocations(test) %>% filter(collocation == "united kingdom")
library(quanteda.corpora)
ukmnfs <- data_corpus_ukmanifestos
test <- corpus_reshape(ukmnfs, to = "sentences")
test.colloc_6b <- textstat_collocations(test) %>% filter(collocation == "united kingdom")
test.colloc_6b
test.colloc_6c <- textstat_collocations(test,  min_count=5)
##Lambda
test.colloc_6c_top10_lam <- test.colloc_6c %>% arrange(desc(lambda)) %>% top_n(10)
a <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/id.Dennis+Schwartz")
View(a)
a <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/id.Dennis+Schwartz")
b <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/rating.Dennis+Schwartz")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/subj.Dennis+Schwartz")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/subj.Dennis+Schwartz")
help("read.table")
c <- scan("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/subj.Dennis+Schwartz")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/subj.Dennis+Schwartz", sep = "\n")
View(c)
View(c)
a <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/id.James+Berardinelli")
b <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/Dennis+Schwartz/rating.James+Berardinelli")
b <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/rating.James+Berardinelli")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/subj.James+Berardinelli", sep = "\n")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/subj.James+Berardinelli", sep = "\n", quote = "")
View(c)
reviews <- cbind(a, b, c)
View(reviews)
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
b <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/label.4class.James+Berardinelli")
c <- read.table("/Users/christianbaehr/Downloads/scale_data/scaledata/James+Berardinelli/subj.James+Berardinelli", sep = "\n", quote = "")
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
View(reviews)
table(reviews$rating)
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/id.%s"
sprintf(root, "James+Berardinelli")
sprintf(root, c("James+Berardinelli", "James+Berardinelli"))
help(sprintf)
sprintf(root, "James+Berardinelli", "James+Berardinelli")
join <- function(name, class) {
a <- read.table(root, class, name, name)
b <- read.table(root, class, name, name)
c <- read.table(root, class, name, name)
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli")
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/%s.%s"
join <- function(name) {
a <- read.table(root, name, "id", name)
b <- read.table(root, name, "label", name)
c <- read.table(root, name, "subj", name)
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli")
lapply(names, join)
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/%s.%s"
join <- function(name) {
a <- read.table(sprintf(root, name, "id", name))
b <- read.table(sprintf(root, name, "label", name))
c <- read.table(sprintf(root, name, "subj", name))
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli")
lapply(names, join)
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/%s.%s"
join <- function(name) {
a <- read.table(sprintf(root, name, "id", name))
b <- read.table(sprintf(root, name, "label.4class", name))
c <- read.table(sprintf(root, name, "subj", name))
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli")
lapply(names, join)
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/%s.%s"
join <- function(name) {
a <- read.table(sprintf(root, name, "id", name))
b <- read.table(sprintf(root, name, "label.4class", name))
c <- read.table(sprintf(root, name, "subj", name), sep = "\n", quote = "")
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli")
lapply(names, join)
root <- "/Users/christianbaehr/Downloads/scale_data/scaledata/%s/%s.%s"
join <- function(name) {
a <- read.table(sprintf(root, name, "id", name))
b <- read.table(sprintf(root, name, "label.4class", name))
c <- read.table(sprintf(root, name, "subj", name), sep = "\n", quote = "")
reviews <- cbind(a, b, c) |>
setNames(c("id", "rating", "text"))
return(reviews)
}
names <- c("James+Berardinelli", "Dennis+Schwartz", "Scott+Renshaw", "Steve+Rhodes")
out <- lapply(names, join)
out <- do.call(rbind, out)
View(out)
table(out$rating)
write.csv(out, "/Users/christianbaehr/Desktop/movie_reviews.csv", row.names = F)
## load packages
pacman::p_load(quanteda, quanteda.corpora, readtext, quanteda.textmodels,
quanteda.textplots, dplyr)
remotes::install_github("leeper/pdfcount")
1
help(perplexity)
help(perplexity)
setwd("/Users/christianbaehr/Documents/GitHub/POL504_precept_2023/data/")
## some random processes in the code
set.seed(123)
pacman::p_load(conText,
quanteda,
dplyr,
text2vec,
ggplot2)
## Embeddings
#glove <- readRDS(url("https://www.dropbox.com/scl/fi/mq3su57tw3jniszezrts6/glove.rds?rlkey=ocwtgbfvxz4ytk6w7j8je78nr&dl=0"))
glove <- readRDS("/Users/christianbaehr/Dropbox/POL504/glove.rds")
dim(glove)
rownames(glove)
## Transformation matrix
khodak <- readRDS("khodakA.rds")
dim(khodak)
## corpus of Congressional Records from conText
corpus <- cr_sample_corpus
## corpus of Congressional Records from conText
corpus <- cr_sample_corpus
docvars <- docvars(corpus) %>%
mutate(party_dem = ifelse(party == "D", 1, 0),
gender_female = ifelse(gender == "F", 1, 0),
interaction = gender_female * party_dem)
docvars(corpus) <- docvars
## tokenize corpus
## remove unnecessary (i.e. semantically uninformative) elements
toks <- tokens(corpus,
remove_punct=T,
remove_symbols=T,
remove_numbers=T,
remove_separators=T)
# remove stopwords, terms with fewer than three characters
toks_nostop <- tokens_select(toks,
pattern = stopwords("en"),
selection = "remove",
min_nchar=3)
## only keep features appearing at least 5 times
feats <- dfm(toks_nostop,
tolower=T,
verbose = FALSE) %>%
dfm_trim(min_termfreq = 5) %>%
featnames()
## padding replaces infrequent tokens with empty("") vector instead of
## removing them entirely. This retains original spacing, which is important
## for embedding windows.
toks_nostop_feats <- tokens_select(toks_nostop,
feats,
padding = TRUE)
immig_toks <- tokens_context(x = toks_nostop_feats,
pattern = "immigr*",
window = 6L)
head(immig_toks)
head(immig_toks)
length(immig_toks)
head(docvars(immig_toks), 3)
## build document-feature matrix
immig_dfm <- dfm(immig_toks)
dim(immig_dfm)
immig_dfm[1:3,]
## build a document-embedding-matrix.
## We embed a document by multiplying each of its feature counts with their
## corresponding pre-trained feature-embeddings, column-averaging the resulting vectors,
## and multiplying by the transformation matrix.
immig_dem <- dem(x = immig_dfm,
pre_trained = glove,
transform = TRUE,
transform_matrix = khodak,
verbose = TRUE)
dim(immig_dem)
head(immig_dem)
immig_wv <- matrix(colMeans(immig_dem),
ncol = ncol(immig_dem)) %>%  `rownames<-`("immigration")
dim(immig_wv)
immig_wv
## to get group-specific embeddings, average within party
immig_wv_party <- dem_group(immig_dem,
groups = immig_dem@docvars$party)
dim(immig_wv_party)
immig_wv_party
set.seed(123)
model1 <- conText(formula = immigration ~ party + gender,
data = toks_nostop_feats,
pre_trained = glove,
transform = T, transform_matrix = khodak,
jackknife = F,
bootstrap = T, num_bootstraps = 100,
permute = T, num_permutations = 100,
window = 6, case_insensitive = T,
verbose = F)
## why a single coefficient per covariate?
model1@normed_coefficients
## why a single coefficient per covariate?
model1@normed_coefficients
## D-dimensional beta coefficients
## the intercept in this case is the ALC embedding for female Democrats
## beta coefficients can be combined to get each group's ALC embedding
DF_wv <- model1['(Intercept)',] # (D)emocrat - (F)emale
DM_wv <- model1['(Intercept)',] + model1['gender_M',] # (D)emocrat - (M)ale
RF_wv <- model1['(Intercept)',] + model1['party_R',]  # (R)epublican - (F)emale
RM_wv <- model1['(Intercept)',] + model1['party_R',] + model1['gender_M',] # (R)epublican - (M)ale
## nearest neighbors
nns(rbind(DF_wv,DM_wv), N = 10, pre_trained = glove, candidates = model1@features)
nns(rbind(DF_wv,RM_wv), N = 10, pre_trained = glove, candidates = model1@features)
## model with interaction also possible, but need to calculate interaction ourselves
model2 <- conText(formula = immigration ~ party_dem + gender_female + interaction,
data = toks_nostop_feats,
pre_trained = glove,
transform = T, transform_matrix = khodak,
jackknife = F,
bootstrap = T, num_bootstraps = 100,
permute = T, num_permutations = 100,
window = 6, case_insensitive = T,
verbose = F)
## plot the (normed) model coefficients by group
ggplot(data = model2@normed_coefficients,
aes(x = factor(coefficient, levels = c("gender_female", "party_dem", "interaction")), y = normed.estimate)) +
geom_point() +
geom_errorbar(aes(ymin = normed.estimate - 1.96*std.error,
ymax = normed.estimate + 1.96*std.error),
width = 0.5) +
labs(x = "") +
theme_bw()
DF_wv <- model2['(Intercept)',] + model2['gender_female',] + model2['party_dem',] + model2['interaction',]# (D)emocrat - (F)emale
DM_wv <- model2['(Intercept)',] + model2['party_dem',] # (D)emocrat - (M)ale
RF_wv <- model2['(Intercept)',] + model2['gender_female',]  # (R)epublican - (F)emale
RM_wv <- model2['(Intercept)',] # (R)epublican - (M)ale
## nearest neighbors (in GloVe)
nns(rbind(DF_wv,RM_wv), N = 10, pre_trained = glove, candidates = model2@features)
